<!DOCTYPE html>
<html>
<meta name="viewport" content="width=device-width,initial-scale=1.0">
<head>
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-115491079-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-115491079-1');
</script>

    <title>Preparing Custom Dataset for Training YOLO Object Detector</title>
    <meta name="description" content="Preparing Custom Dataset for Training YOLO Object Detector with DarkNet to detect new object categories that are not covered in pre-trained models. "/>
    <meta name="twitter:card" content="summary_large_image"/>
    <meta name="twitter:site" content="arunponnusamy.com"/>
    <meta name="twitter:creator" content="@ponnusamy_arun"/>
    <meta name="twitter:image:src" content="http://www.arunponnusamy.com/images/preparing-custom-dataset-for-training-yolo-object-detector/object-detection-illustration.png"/>

    <meta charset="utf-8"/>
    <meta property="og:title" content="Preparing Custom Dataset for Training YOLO Object Detector">
    <meta property="og:image" content="images/preparing-custom-dataset-for-training-yolo-object-detector/object-detection-illustration.png">
    <meta property="og:description" content="Preparing Custom Dataset for Training YOLO Object Detector with DarkNet to detect new object categories that are not covered in pre-trained models.">
    <meta property="og:url" content="arunponnusamy.com">

    <link rel="stylesheet" type="text/css" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css">
    <link rel="stylesheet" type="text/css" href="main.css">
    
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=PT+Sans:400,400italic,700|Abril+Fatface">
</head>

<body>

<header>
    <div class="container">
        <div class="row">
            <h2 class="col-sm-4 text-left">
            <a href="index.html">Arun Ponnusamy</a></h2>
            <nav class="col-sm-8 text-right">
            <h3>
            <a href="index.html">Home</a></h3>
	    <h3>
            <a href="about.html">About</a></h3>
	    <h3><a href="https://book.arunponnusamy.com/computer-vision-essentials/">Book</a></h3>
            <h3>
	    <a href="https://github.com/arunponnusamy/cvlib">Project</a></h3>
	    <h3><a href="https://arunponnusamy.substack.com">Newsletter</a></h3>
            <h3>
            <a href="ArunPonnusamy-CV.pdf">CV</a></h3>
            <h3>
            <a href="contact.html">Contact</a></h3>            
            </nav>
        </div>
    </div>

</header>

<section>
    <div>
        <h1 class="col-sm-offset-3 col-sm-6 col-sm-offset-3">Preparing Custom Dataset for Training YOLO Object Detector</h1>
	<p id="author" class="col-sm-offset-3 col-sm-6 col-sm-offset-3">06 Oct 2019 Arun Ponnusamy<p>
        <img class="col-sm-offset-2 col-sm-8 col-sm-offset-2" src="images/preparing-custom-dataset-for-training-yolo-object-detector/object-detection-illustration.png"/>
        <p class="col-sm-offset-4 col-sm-4 col-sm-offset-4" id="source">Source: <a href="https://tryolabs.com/resources/introductory-guide-computer-vision/">Tryo labs</a></p>
	
	<p></p>
	<p class="col-sm-offset-3 col-sm-6 col-sm-offset-3">In an earlier <a href="https://www.arunponnusamy.com/yolo-object-detection-opencv-python.html">post</a>, we saw how to use a pre-trained YOLO model with OpenCV and Python to detect objects present in an image. (also known as running 'inference')</p>

	<p class="col-sm-offset-3 col-sm-6 col-sm-offset-3">As the word 'pre-trained' implies, the network has already been trained with a dataset containing a certain number of classes (object categories). For example, the model we used in the previous post was trained on the <a href="http://cocodataset.org">COCO dataset</a> which contains images with 80 different object <a href="https://github.com/arunponnusamy/object-detection-opencv/blob/master/yolov3.txt"> categories</a>.</p>

	<p class="col-sm-offset-3 col-sm-6 col-sm-offset-3">Basically somebody else trained the network on this dataset and made the learned weights available on the internet for everyone to use. In this case the trained weights for YOLO was from the original author of YOLO, <a href="https://pjreddie.com">Joseph Redmon</a>. (Thanks Joseph!)</p>

	<p class="col-sm-offset-3 col-sm-6 col-sm-offset-3">But what if you want to detect an object that is not part of the classes that the pre-trained model was trained on. Well, then you have to train the model with a dataset containing images with the object you are interested in. This involves a lot more work compared to just running inference with a pre-trained model.</p>

	<p class="col-sm-offset-3 col-sm-6 col-sm-offset-3">I will be covering the end to end process of training a custom object detector with YOLO in a series of blog posts starting with this post.</p>

	<p class="col-sm-offset-3 col-sm-6 col-sm-offset-3">1. Collecting dataset and annotating/labeling. (this post)</p>

	<p class="col-sm-offset-3 col-sm-6 col-sm-offset-3">2. Installing DarkNet, setting up the environment and training.</p>

	<p class="col-sm-offset-3 col-sm-6 col-sm-offset-3">3. Multiple ways of running YOLO inference.</p>

	<p class="col-sm-offset-3 col-sm-6 col-sm-offset-3">Let's get started.</p>

	<p class="col-sm-offset-3 col-sm-6 col-sm-offset-3 text-center">.    .    .</p>


<h2 class="col-sm-offset-3 col-sm-6 col-sm-offset-3">Gathering Data</h2>


	<p class="col-sm-offset-3 col-sm-6 col-sm-offset-3">One of the crucial parts of building machine learning systems is gathering high quality dataset. You can expect to spend significant amount of time on data. It is essential because our model is only as good as the data it learns from.</p>

	<p class="col-sm-offset-3 col-sm-6 col-sm-offset-3">In this series, we will try build an object detector that is trained to detect people wearing 'helmets' in the scene.</p>
		
	<p class="col-sm-offset-3 col-sm-6 col-sm-offset-3">So how do we teach a machine to detect helmets ? As you might have guessed, by showing a lot of examples.</p>
	
<p class="col-sm-offset-3 col-sm-6 col-sm-offset-3">There are various ways to collect data. When it comes to images, one of the easiest ways for an individual to collect images is to use Google Image Search.</p>

<img class="col-sm-offset-2 col-sm-8 col-sm-offset-2" src="images/preparing-custom-dataset-for-training-yolo-object-detector/google_search.png"/>

<p class="col-sm-offset-4 col-sm-4 col-sm-offset-4" id="source">Source: <a href="https://www.google.co.in/search?q=people+wearing+helmet&source=lnms&tbm=isch&sa=X&ved=0ahUKEwjxjIjR4bHkAhU9IbcAHaU2AV4Q_AUIESgB&biw=1440&bih=821">Google Image Search</a></p>

<p class="col-sm-offset-3 col-sm-6 col-sm-offset-3">But downloading the images show up in search results one by one manually is a tedious task. Luckily there are some tools to help. Came across a chrome extension called <a href="https://chrome.google.com/webstore/detail/download-all-images/ifipmflagepipjokmbdecpmjbibjnakm?hl=en">Download All Images</a> which, as the name says, downloads all images present in a web page with a click of a button. </p>

<p class="col-sm-offset-3 col-sm-6 col-sm-offset-3">Scroll down on the search results page as much as you want. The tool will capture the images till the length you have scrolled. (Please look at the individual image sources for copyright related information) </p>

<img class="col-sm-offset-3 col-sm-6 col-sm-offset-3" src="images/preparing-custom-dataset-for-training-yolo-object-detector/download_all_images_chrome_ext.png"/>

<p class="col-sm-offset-4 col-sm-4 col-sm-offset-4" id="source">Source: <a href=https://chrome.google.com/webstore/detail/download-all-images/ifipmflagepipjokmbdecpmjbibjnakm?hl=en">Chrome Web Store</a></p>

<p class="col-sm-offset-3 col-sm-6 col-sm-offset-3">There could be other ways to bulk download images from Google Search, this one seemed dead-simple.</p>

<p class="col-sm-offset-3 col-sm-6 col-sm-offset-3">Once the images are downloaded, be sure to go through them and remove any irrelevant images.</p>
		
<h2 class="col-sm-offset-3 col-sm-6 col-sm-offset-3">Labeling Data</h2>
     
<p class="col-sm-offset-3 col-sm-6 col-sm-offset-3">Once we gather data, next step is to label / annotate them. In the context of object detection, labeling means drawing bounding boxes around the objects that we are interested in detecting in the images and associating them with corresponding object classes / categories so that we can show it to the machine clearly. </p>

<img class="col-sm-offset-4 col-sm-4 col-sm-offset-4" src="images/yolo-object-detection-opencv-python/yolo-object-detection.jpg"/>

<p class="col-sm-offset-4 col-sm-4 col-sm-offset-4" id="source">Source: <a href="https://github.com/pjreddie/darknet/blob/master/data/dog.jpg">DarkNet</a></p>


<p class="col-sm-offset-3 col-sm-6 col-sm-offset-3">This is the most labor intensive part of the process. We need to go through the images one by one and label the objects in each image manually.</p>

<p class="col-sm-offset-3 col-sm-6 col-sm-offset-3">There are lot of tools available to help us annotate the images. One of the widely used Open Source tools is <a href="https://github.com/tzutalin/labelImg">LabelImg</a>.</p>

<img class="col-sm-offset-3 col-sm-6 col-sm-offset-3" src="images/preparing-custom-dataset-for-training-yolo-object-detector/labelimg.jpg"/>

<p class="col-sm-offset-4 col-sm-4 col-sm-offset-4" id="source">Source: <a href="https://github.com/tzutalin/labelImg">LabelImg</a></p>


<p class="col-sm-offset-3 col-sm-6 col-sm-offset-3">Good thing about LabelImg is, it lets us save the annotations directly into YOLO format. Some tools don't directly do that. We need to convert the annotations ourselves into the format YOLO requires.</p>

<p class="col-sm-offset-3 col-sm-6 col-sm-offset-3">You can easily install LabelImg with pip from the Terminal.</p>

<code class="col-sm-offset-3 col-sm-6 col-sm-offset-3">pip install labelimg</code>

<p class="col-sm-offset-3 col-sm-6 col-sm-offset-3">You might want to use pip3 if pip is linked to Python 2.x . You can check this using</p> 

<code class="col-sm-offset-3 col-sm-6 col-sm-offset-3">pip --version</code>

<p class="col-sm-offset-3 col-sm-6 col-sm-offset-3">Once labelImg is successfully installed, launch it by typing</p>

<code class="col-sm-offset-3 col-sm-6 col-sm-offset-3">labelImg [path to image] [classes file] </code>

<p class="col-sm-offset-3 col-sm-6 col-sm-offset-3"><i>[path to image]</i> is the path to an image in the directory containing the downloaded helmet images we are going to label.</p>

<p class="col-sm-offset-3 col-sm-6 col-sm-offset-3"><i>[classes file]</i> is the file where we list the object classes that we are going to label. We haven't created it yet. Let's do that now.</p>

<p class="col-sm-offset-3 col-sm-6 col-sm-offset-3">Create a text file and add 'helmet' in the file. Since we are training for only this category thats all we need to add. If we are going to label 10 different objects, then we should all of them in this file.</p>

<p class="col-sm-offset-3 col-sm-6 col-sm-offset-3">Once the file is created, launch labelImg. For example,</p>

<code class="col-sm-offset-3 col-sm-6 col-sm-offset-3">labelImg /home/Downloads/helmet_images/image_1.jpg classes.txt </code>

<p class="col-sm-offset-3 col-sm-6 col-sm-offset-3">Once the window is opened, you can click 'Open Dir' button on the left panel and select the directory where all of the helmet images are saved. It will bring all of the images to labelImg so that we can go through one by one.</p>

<p class="col-sm-offset-3 col-sm-6 col-sm-offset-3">After all the images are loaded, we can start labeling the images.</p>
<blockquote class="col-sm-offset-3 col-sm-6 col-sm-offset-3"><p>Change the annotation format to YOLO from PASCAL VOC on the left panel before proceeding.</p></blockquote>

<iframe width="560" height="315" src="https://www.youtube.com/embed/zSda1AoUTkc" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen class="col-sm-offset-3 col-sm-6 col-sm-offset-3"></iframe>

<h3 class="col-sm-offset-3 col-sm-6 col-sm-offset-3">Keyboard shortcuts</h3>

<p class="col-sm-offset-3 col-sm-6 col-sm-offset-3"><code> W </code>  -   to start creating bounding box</p>

<p class="col-sm-offset-3 col-sm-6 col-sm-offset-3"><code> Ctrl + S </code>  -   to save the bounding boxes and labels</p>

<p class="col-sm-offset-3 col-sm-6 col-sm-offset-3"><code> D </code>  -   next image</p>

<p class="col-sm-offset-3 col-sm-6 col-sm-offset-3"><code> A </code>  -   previous image</p>

<p class="col-sm-offset-3 col-sm-6 col-sm-offset-3">Complete list of shortcuts can be found <a href="https://github.com/tzutalin/labelImg#hotkeys">here</a>. But these are the shortcuts I found myself using frequently.</p>

<p class="col-sm-offset-3 col-sm-6 col-sm-offset-3"> When you save the labels after each image with 'Ctrl + S', labelImg creates a text file for each image with the same name as the image. All the annotation details for that particular image is saved in that file.</p> 

<p class="col-sm-offset-3 col-sm-6 col-sm-offset-3">For example 'image_1.jpg' will have a corresponding 'image_1.txt' in the same directory. (you can change the directory if you wish using 'Change Save Dir' in the left panel)</p>

<img class="col-sm-offset-3 col-sm-6 col-sm-offset-3" src="images/preparing-custom-dataset-for-training-yolo-object-detector/txt_file.jpg"/>

<p class="col-sm-offset-4 col-sm-4 col-sm-offset-4" id="source">Saved annotations in YOLO format</p>

<p class="col-sm-offset-3 col-sm-6 col-sm-offset-3"> If you are wondering what these cryptic numbers are, these are actually stored in a specific format usually known as 'YOLO format'.</p>

<p class="col-sm-offset-3 col-sm-6 col-sm-offset-3"><code>object-id center_x center_y width height</code></p>

<p class="col-sm-offset-3 col-sm-6 col-sm-offset-3"> <code>object-id</code> represents the number corresponding to the object category which we listed in <i>'classes.txt'</i> earlier.</p>

<p class="col-sm-offset-3 col-sm-6 col-sm-offset-3"> <code>center_x and center_y</code> represents the center point of the bounding box. But they are normalized to range between 0 and 1 by dividing by the width and height of the image.</p> 

<p class="col-sm-offset-3 col-sm-6 col-sm-offset-3">For example, (0.25,0.75) is the point located at 25% of the width and 75% of the height. We can multiply this number (0.25,0.75) by the original width and height of the image to get the real point. In fact, we will be doing this at the end after inference to draw the predictions on the image.</p>

<p class="col-sm-offset-3 col-sm-6 col-sm-offset-3">Generally it is easier for the network to predict values between 0 and 1 than random coordinate values.</p>

<p class="col-sm-offset-3 col-sm-6 col-sm-offset-3"> <code>width and height</code> represents the width and height of the bounding box. Again normalized to the range 0 to 1 dividing by the original width and height of the image.</p> 

<p class="col-sm-offset-3 col-sm-6 col-sm-offset-3">The generated annotation text file will contain each line like the above for each bounding in the image and one text file for each image.</p>

<h2 class="col-sm-offset-3 col-sm-6 col-sm-offset-3">Creating necessary files</h2>

<p class="col-sm-offset-3 col-sm-6 col-sm-offset-3"> Apart from the annotations, there are few necessary files related to data which DarkNet expects for training. Let's create them now.

<p class="col-sm-offset-3 col-sm-6 col-sm-offset-3"><code>classes.names</code> - this is same as the 'classes.txt' file which we used earlier with LabelImg containing the object categories (in our case just 'helmet') just with '.names' extension. So you can just reuse/rename the file to 'classes.names'.</p>

<p class="col-sm-offset-3 col-sm-6 col-sm-offset-3"><code> train.txt</code> - DarkNet expects a text file listing all the images that are going to be used for training. Generally people use 60-90% of the total dataset for training and keep the remaining for testing/validation. There is no real consensus here on the numbers. It varies depending on the situation.</p>

<p class="col-sm-offset-3 col-sm-6 col-sm-offset-3">In this series of posts, I will be using a small amount of data since I couldn't label so much. Feel free to change the split  depending upon how much labeled data you have.</p>

<p class="col-sm-offset-3 col-sm-6 col-sm-offset-3"><code>ls "$PWD/"*.jpg | head -100 > train.txt</code></p>

<p class="col-sm-offset-3 col-sm-6 col-sm-offset-3">I have a total of 120 images labeled (which is very less actually). So I am using 100 images for training and 20 images for validation. Go to the directory where you have the helmet images and run the above command. It should create a text file listing the paths of the first 100 images in the directory. (Feel free to change the number as you see fit)</p>

<p class="col-sm-offset-3 col-sm-6 col-sm-offset-3"><code> test.txt</code> - this file will have the list of the images that will be used for validation.</p>

<p class="col-sm-offset-3 col-sm-6 col-sm-offset-3"><code>ls "$PWD/"*.jpg | tail -20 > test.txt</code></p>

<p class="col-sm-offset-3 col-sm-6 col-sm-offset-3">Go to the directory where you have the helmet images and run the above command. It should create a text file listing the paths of the last 20 images in the directory. (Again, feel free to change the number as you see fit)</p>

<p class="col-sm-offset-3 col-sm-6 col-sm-offset-3">Once these text files are created you can move them from the images directory to appropriate place in the project directory you are working on or you can just leave it there.</p>


<p class="col-sm-offset-3 col-sm-6 col-sm-offset-3"><code>obj.data</code> - this file will contain the following lines.</p>

<code class="col-sm-offset-3 col-sm-6 col-sm-offset-3">
	<p>classes= 1</p>
	<p>train  = /path/to/train.txt</p>
	<p>valid  = /path/to/test.txt</p>
	<p>names = /path/to/classes.names</p>
	<p>backup = backup/ </p>
</code>

<p class="col-sm-offset-3 col-sm-6 col-sm-offset-3">First line is the number of object classes that we are training the network to detect. 'backup' is the place where DarkNet will save the trained weights for us.

<h2 class="col-sm-offset-3 col-sm-6 col-sm-offset-3">Summary</h2>

<p class="col-sm-offset-3 col-sm-6 col-sm-offset-3">In this post, we looked at collecting data for a particular object (helmet), annotating them using labelImg, what do the values in the annotation file actually means, created train / test split and we also created necessary files in the format DarkNet expects to train the YOLO model.</p>

<p class="col-sm-offset-3 col-sm-6 col-sm-offset-3">I hope you enjoyed today's post. In the next post, I'll explain how to actually train the object detection model (YOLO) using DarkNet. Be sure to subscribe to get notified when  the next post is published.</p>

<p class="col-sm-offset-3 col-sm-6 col-sm-offset-3"><script async data-uid="c3fe30048c" src="https://expert-musician-1659.ck.page/c3fe30048c/index.js"></script></p>

<p class="col-sm-offset-3 col-sm-6 col-sm-offset-3">Feel free to share your thoughts in the comments or you can reach out on twitter <a href="https://twitter.com/ponnusamy_arun">@ponnusamy_arun</a>.</p>

<p class="col-sm-offset-3 col-sm-6 col-sm-offset-3">Cheers.</p>
	
<h3 class="col-sm-offset-3 col-sm-6 col-sm-offset-3"><b>Recent posts</b></h3>
<p class="col-sm-offset-3 col-sm-6 col-sm-offset-3"><a href="https://www.arunponnusamy.com/yolo-object-detection-opencv-python.html">YOLO Object Detection with OpenCV and Python</a></p>
<p class="col-sm-offset-3 col-sm-6 col-sm-offset-3"><a href="https://www.arunponnusamy.com/cnn-face-detector-dlib.html">CNN based face detector from dlib</a></p>
<p class="col-sm-offset-3 col-sm-6 col-sm-offset-3"><a href="https://www.arunponnusamy.com/deep-learning-setup-macos.html">Setting up deep learning environment the easy way on macOS High Sierra</a></p>


   </div>

</section>

<div id="disqus_thread" class="col-sm-offset-3 col-sm-6 col-sm-offset-3"></div>
<script class="col-sm-offset-3 col-sm-6 col-sm-offset-3">

/**
*  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
*  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*/
/*
var disqus_config = function () {
this.page.url = 'http://arunponnusamy.com/preparing-custom-dataset-for-training-yolo-object-detector.html';  // Replace PAGE_URL with your page's canonical URL variable
this.page.identifier = 'preparing-custom-dataset-for-training-yolo-object-detector'; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
};
*/
(function() { // DON'T EDIT BELOW THIS LINE
var d = document, s = d.createElement('script');
s.src = 'https://arunponnusamy.disqus.com/embed.js';
s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
                            

<footer class="col-sm-offset-3 col-sm-6 col-sm-offset-3">
    <div class="row">
        <p class="col-sm-4">&copy; 2019 Arun Ponnusamy</p>
        <ul class="col-sm-8">
            <li class="col-sm-1"><a href="https://github.com/arunponnusamy"><img src="https://image.flaticon.com/icons/png/512/25/25231.png"></a></li>  
            <li class="col-sm-1"><a href="https://twitter.com/ponnusamy_arun"><img src="https://www.w3.org/2008/site/images/Twitter_bird_logo_2012.svg"></a></li>  
            <li class="col-sm-1"><a href="https://www.linkedin.com/in/arun-ponnusamy/"><img src="https://norfipc.com/img/logos/logotipo-oficial-linkedin-2014.png"></a></li>
            <li class="col-sm-1"><a href="https://www.quora.com/profile/Arun-Ponnusamy-2"><img src="http://cdn.embed.ly/providers/logos/quora.png"></a></li>
            <li class="col-sm-1" id="medium"><a href="https://medium.com/@arunponnusamy"><img src="http://www.stickpng.com/assets/images/5841c47ba6515b1e0ad75aa3.png"></a></li>  
    </div>
</footer>

</body>

</html>
